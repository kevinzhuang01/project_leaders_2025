---
layout: profile
name: Hattie Lyons
organization: Loyola University Chicago
department: Computer Science
project_title: A Modular Architecture for Detecting Contextual Divergence in Large
  Language Models
status: Accept (Confirmed)
abstract: Large Language Models (LLMs) are increasingly being used as agentic systems
  in hyper-specific information domains such as medicine, law, and therapeutic practices.
  However, their capacity to "drift" from their designated context or generate hallucinated
  information undermines trust. This project introduces a modular supervision architecture
  designed to detect and flag hallucinatory drift in LLM outputs. The system utilizes
  multiple layers, namely centroid-based embedding comparison and k-nearest neighbor
  (KNN) classification to evaluate alignment between generated output and source context.
  The toolkit is being developed with a focus on flexibility and usability. Researchers
  can configure supervision layers and evaluation strategies according to their domain
  needs. Prototypes have demonstrated the ability to identify divergence from baseline
  context in conversational LLM pipelines and agentic task-based systems like Anthropic's
  VendingBench simulation. Future versions aim to support broader applications, such
  as scientific/medical document summarization and content moderation. By providing
  a configurable framework for context drift detection, this project highlights the
  importance of reliability, interpretability, and reproducibility in AI research
  software.
academic_interests: My academic interests revolve around the overlap of artificial
  intelligence, systems design, and linguistics/semantics. I am especially fascinated
  by latent space dynamics, algorithmic bias, and AI hallucination. My ongoing research
  focuses on the reliability and interpretability of artificial intelligence systems,
  particularly large language models (LLMs). I am actively building a modular supervision
  software for language models that uses semantic embeddings and data structures to
  detect and flag hallucinatory output in LLM systems. This project reflects my broader
  interest in building configurable, open-source research tools that improve the robustness
  and transparency of AI-driven workflows.
email: hlyons2@luc.edu
citizenship_status: US Citizen
academic_status: Master's Student
website: https://www.linkedin.com/in/hattie-lyons-465a721a6/
image: "/assets/images/profiles/Letter-Number-Files/hattie_lyons_1.jpg"
---

## Academic Interests

My academic interests revolve around the overlap of artificial intelligence, systems design, and linguistics/semantics. I am especially fascinated by latent space dynamics, algorithmic bias, and AI hallucination. My ongoing research focuses on the reliability and interpretability of artificial intelligence systems, particularly large language models (LLMs). I am actively building a modular supervision software for language models that uses semantic embeddings and data structures to detect and flag hallucinatory output in LLM systems. This project reflects my broader interest in building configurable, open-source research tools that improve the robustness and transparency of AI-driven workflows.

