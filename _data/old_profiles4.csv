Submission,Status (This Stage),First/Given Names (first),Last/Family Name (first),Email (first),Company/Institution (first),Department (first),Photograph (first),Website (first),SRP Project Title (first),What is the NAIRR Project Name? (first),What is the HPSF Project Name? (first),Please select all the topical areas that apply to your project: (first),Brief Abstract (200 words) (first),"Desired relevant skills, background, or interests (don't be too prescriptive) (first)",Any other comments (first),Lightning Talk Title (Maximum 10 words),Keywords (Maximum 20 words),Biography (Maximum 200 words)
proj102s1,Accept Wave 2 (Confirmed),Shu,Hu,hu968@purdue.edu,Purdue University,School of Applied and Creative Computing,png Information Type: pngSize: 219KBUploaded: Sep 16MD5: 38996968c22dbe4f8e07f460d98c9ef6Original Name: Shu Hu.png view move to AWS,https://web.ics.purdue.edu/~hu968/,Improving Fairness in Detecting AI-Synthesized Fake Multimedia,Improving Fairness in Detecting AI-Synthesized Fake Multimedia,,Applied Computer Science; Artificial Intelligence and Intelligent Systems; Computer Science; Media and communications; Other Computer and Information Sciences; Other Engineering and Technologies; Performance Evaluation and Benchmarking; Visualization and Human-Computer Systems,"DeepFake, a term increasingly mentioned in the news and social media, refers to highly realistic fake images, and videos created using AI algorithms. Combating DeepFake technology requires a comprehensive strategy that extends well beyond the realm of mere detection, emphasizing the responsible design, development, and deployment of technologies. This also known as responsible forensics, focuses on applying forensic science to digital content ethically, ensuring that actions to identify and mitigate DeepFakes meet high ethical standards and respect for human rights. At the heart of responsible forensics lies the commitment to fairness, especially important in the context of generative AI. It is crucial for detection tools to be crafted and used in ways that prevent unintentional bias against certain individuals or groups, thus upholding justice and equality in the digital realm. Therefore, our goal is to improve fairness in detecting novel DeepFakes.",Python Programming Skill.,,Improving Fairness in Detecting AI-Synthesized Fake Multimedia,DeepFake Detection; Media Forensics; Generalization; AI-generated Media,"Dr. Shu Hu is an assistant professor in the School of Applied and Creative Computing and the Director of the Purdue Machine Learning and Media Forensics (M2) Lab at Purdue University. He was a Post-Doctoral Fellow at the Heinz College of Carnegie Mellon University from 2022 to 2023. He received my Ph.D. degree in Computer Science and Engineering from the University at Buffalo, SUNY in 2022. He is the recipient of the National AI Research Resource (NAIRR) Pilot award (2024), the National Science Foundation CRII Award (2024), the Machine Intelligence Research Outstanding Reviewer Award (2023), and SUNY Buffalo's CSE Best PhD Dissertation Award (2022). His research interests include machine learning, media forensics, and computer vision."
proj104s1,Accept Wave 2 (Confirmed),Armstrong,Aboah,armstrong.aboah@ndsu.edu,North Dakota State University,,,,PRIME: A Foundational Predictive Real-time Intersection Monitoring Engine,PRIME: A Foundational Predictive Real-time Intersection Monitoring Engine,,Applied Computer Science; Artificial Intelligence and Intelligent Systems; Civil Engineering; Computer Science,"Traffic intersections remain critical points of vulnerability in transportation infrastructure, accounting for 20% of vehicular accidents and over 7,000 annual fatalities in the United States. Current intersection monitoring systems, relying on basic motion detection or single-class object detection, struggle with complex scenarios involving multiple road users and varying environmental conditions. This research proposes PRIME (Predictive Real-time Intersection Monitoring Engine), a foundational traffic intersection monitoring algorithm that integrates advanced deep learning techniques for multi-class video object detection and trajectory prediction. This project directly aligns with NAIRR priority areas by creating open-source foundation models for specific applications and utilizing experimental data from sensors and detectors. The proposed system employs a novel object detection architecture with enhanced feature pyramid networks and combines transformer encoders with graph neural networks to achieve robust object detection and accurate 5-second trajectory predictions. Our technical objectives include developing a multi-class detection system with over 95% accuracy, implementing proactive trajectory prediction, and generating anonymous traffic patterns for urban planning. The project will be executed utilizing TACC Frontera GPU resources for model development and training. We will also release our codebase and make our annotated dataset publicly available with the aim of establishing a common foundation for intersection monitoring systems.","A good programming background, like Python. Familiarity with computer vision.",,Towards a safe and smart intersection,"Machine learning, Safety, Intersection, trajectory","I am an Assistant Professor at the North Dakota State University. An ingenious and resourceful Transportation Data Scientist with a proven track record of success in research and hands-on experience developing cutting-edge database solutions, statistical modeling, data products, and computer vision systems aimed at improving transportation system management and operations. Has worked as an architect and application developer on a variety of projects that required the use of data mining and machine learning models to solve large-scale, complex, and difficult transportation problems. I am broadly interested in computer vision and machine learning. My research involves visual reasoning, vision and language, image generation, air taxis, naturalistic studies, and autonomous vehicles."
proj105s1,Accept Wave 2 (Confirmed),Yupeng,Zhang,yupeng@alumni.caltech.edu,"University of California, Los Angeles",,jpg Information Type: jpgSize: 85KBUploaded: Sep 16MD5: 8ff1a162ebcfed07230b96d9a1422d2bOriginal Name: self_2024.jpg view move to AWS,https://orcid.org/0000-0001-7149-451X,Iterative learning for materials and structures,Geometry Effects on Iterative Learning for Multiscale Modeling of History-Dependent Metamaterials,,"Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; Civil Engineering; Computer Science; Geology and Solid Earth Sciences; Informatics, Analytics and Information Science; Infrastructure and Instrumentation; Materials Engineering; Mechanical Engineering; Other Computer and Information Sciences; Other Engineering and Technologies; Statistics and Probability; Visualization and Human-Computer Systems","A big challenge in advancing AI methods that enable scientific discovery is to understand what kinds of data are necessary to obtain accurate and transferable surrogate models. We explored this question in the context of the history-dependent behavior of materials and structures. We introduced an iterative approach where we used a rich arbitrary class of trajectories to train an initial model. We then iteratively updated the class of trajectories with those that arise in large-scale simulation and used transfer learning to update the model. We showed that such an approach converges to a highly accurate surrogate, and one that is transferable. In our current NAIRR Pilot project, we are investigating how geometry influences iterative learning in multiscale modeling of history-dependent metamaterials. Through the Sustainable Research Pathways (SRP) Program, we aim to develop AI/ML-based digital twins of physical materials and structures under various boundary conditions. The digital twins will serve as fast and reliable surrogates for multiscale modeling, optimization, and inverse design. This research connects AI/ML with mechanics, materials, structures, and design. Participants will gain experience with machine learning, numerical simulation, with applications spanning mechanical civil, and materials engineering.","Participants with interests in artificial intelligence, machine learning, numerical modeling, or materials / structures / mechanics are encouraged to apply. A background in engineering, computer science, applied math, applied statistics, or related fields is helpful, but not required. Curiosity and enthusiasm are especially valuable.",,AI/ML model for complex mechanical systems,"solid mechanics, materials characterization, Bayesian statistics, inverse problems, AI/ML, multiscale modeling of complex system, thermo-magneto-mechanical couplings.","Yupeng Zhang is currently with the Department of Mechanical and Aerospace Engineering at the University of California, Los Angeles (UCLA). He received his Ph.D. in Materials Science and Engineering from Texas A&M University, specializing in solid mechanics, Bayesian statics, and materials characterization, followed by postdoctoral positions at Northwestern University, and the California Institute of Technology. Zhang is a member of the ASCE - EMI Machine Learning in Mechanics Committee and the recipient of NAIRR grant as a single PI, the Future Faculty Symposium Travel Award from the Society of Engineering Science in 2023, the Clearfield Materials Fellowship at Texas A&M University, and the Mitacs Globalink Research internship from Mitacs Canada. Zhang has mentored over ten graduate and undergraduate students and serves as a reviewer for such as Journal of Applied Mechanics (ASME), Journal of Engineering Mechanics (ASCE), Mechanics of Materials (Elsevier), European Journal of Mechanics / A Solids (Elsevier), Journal of Materials Research (Springer), Journal of Engineering Materials and Technology (ASME), Experimental Mechanics (Society for Experimental Mechanics, SEM), Physics of Fluids(AIP). His research interests include solid mechanics, materials characterization Bayesian statistics, inverse problems, and AI/machine learning, focusing on multiscale modeling of complex system, and thermo-magneto-mechanical couplings."
proj106s1,Accept Wave 2 (Confirmed),Zhuangdi,Zhu,zzhu24@gmu.edu,George Mason University,,,http://zhuangdizhu.github.io/,Developing Engaging AI Chatbots to Enhance Senior Well-being,Developing Engaging AI Chatbots to Enhance Senior Well-being,,Applied Computer Science; Artificial Intelligence and Intelligent Systems; Health Sciences,"The mental well-being of the elderly population is a growing concern. According to the World Health Organization, approximately $14\%$ of adults aged 60 and over experience mental disorders, and the issues are often compounded by loneliness and social isolation. Large Language Models (LLMs) offer a promising solution by enabling meaningful, multi-turn dialogues. Leveraging the advanced natural language processing utility of LLMs, we aim to develop an AI chatbot specifically designed for seniors. Our research goal is to create an LLM-empowered dialog system that ensures engaging conversations for senior participants, thus supporting their cognitive functions and offering accessible interaction tools alongside traditional human interviews. It also underpins research for crucial applications such as detecting Mild Cognitive Impairment and intervening in the early stages of dementia.",Successfully led at least one project in a relevant domain or involving large language models (LLMs). + First-author experience in submitting research papers to leading ML/AI conferences. + Strong background in LLM-focused research programming and prompt engineering.,,Developing AI Chatbots for Improving Cognitive Health,Agentic AI Chatbot Cognitive Health User Study,"Zhuangdi Zhu is an assistant professor at the Department of Cyber Security Engineering of George Mason University. Prior to that, she worked as a senior Data & Applied Scientist for Microsoft. I received my Ph.D. degree from the Department of Computer Science and Engineering at Michigan State University. Her research centers around Accountable AI, through efforts in two directions: (1) advancing interactive AI to learn and reason over long horizons through principled Reinforcement Learning, and (2) decentralizing AI to the edge while balancing security, privacy, and efficiency."
proj107s1,Accept Wave 2 (Confirmed),Naeemul,Hassan,nhassan@umd.edu,University of Maryland,College of Information,,,AI for Quality Healthcare Information,Advancing Explainable LLM to Bridge the Knowledge-Practice Gap in Healthcare Communication,,"Applied Computer Science; Artificial Intelligence and Intelligent Systems; Computer Science; Health Sciences; Informatics, Analytics and Information Science; Media and communications; Other Computer and Information Sciences; Other Medical Sciences; Statistics and Probability; Visualization and Human-Computer Systems","This project investigates how large language models (LLMs) can help bridge the gap between scientific best practices and healthcare journalism. Research shows that news coverage of medical treatments often omits critical details such as harms, evidence quality, or alternatives leading to public misunderstanding and even harmful health decisions. While frameworks exist to assess the quality of healthcare news, they are labor-intensive and not scalable. Our project explores the potential of LLMs (e.g., GPT models, LLaMA) to automatically evaluate healthcare news articles against science-informed criteria, provide explainable feedback, and support journalists in improving reporting quality. We will test models on a curated dataset of 2,000 expert-annotated healthcare articles and extend the evaluation to larger datasets.","The project blends computational journalism, natural language processing, and human-computer interaction to advance both AI explainability and its practical applications in healthcare communication. Relevant expertise- Interest in AI/LLMs, computational journalism, or healthcare communication Familiarity with Python, machine learning, or NLP libraries (e.g., Hugging Face, OpenAI API) Experience with data annotation, text analysis, or evaluation frameworks Curiosity about interdisciplinary research that combines computer science, journalism, and public health Strong analytical and communication skills, with an openness to learning across fields",,Advancing Explainable LLM to Bridge the Knowledge-Practice Gap in Healthcare,Artificial Intelligence; Large Language Model; Health Information; Natural Language Processing,"Dr. Naeemul Hassan is an Associate Professor in the Philip Merrill College of Journalism at the University of Maryland, jointly appointed with the College of Information Studies (iSchool). His research lies at the intersection of computational journalism, data science, and artificial intelligence, focusing on combating misinformation and enhancing the transparency and efficiency of news production. Dr. Hassan’s work combines natural language processing, machine learning, and human-centered approaches to improve how information is produced, verified, and consumed."
proj109s1,Accept Wave 2 (Confirmed),Xiao,Wang,xiaowangatpurdue@gmail.com,Oak Ridge National Laboratory,,jpg Information Type: jpgSize: 756KBUploaded: Sep 17MD5: 3e4ead91c3b1185356294792f54a5deeOriginal Name: IMG_0151.JPG view move to AWS,https://www.ornl.gov/staff-profile/xiao-wang,Computing-Efficient Training for Large-Scale Vision Transformer Foundation Models,Computing-Efficient Training for Large-Scale Vision Transformer Foundation Models,,Applied Computer Science; Artificial Intelligence and Intelligent Systems; Computer Science,"Vision Transformer (ViT) is a powerful AI architecture for computer vision that is used by most imaging foundation models due to its effectiveness in discerning complex visual patterns across many tasks. However, training large-scale ViT foundation models requires considerable computing resources, leading to a significant energy footprint for training. For example, Open-AI’s SORA video generator model was trained on more than 10,000 NVIDIA H-100 GPUs and the training took more than a month on a supercomputer. The energy consumption for training SORA was equivalent to the total annual energy consumption of 300 US households. This one-year project aims to improve ViT scaling algorithms computing efficiency, reducing AI development cycle and training time. We will develop a training framework optimized for hardware-conscious scaling and computing efficiency, specifically tailored for large-scale ViT models.","AI, efficient computing",,Energy Efficient Vision Transformer Training Framework For Exascale Foundation Models,"vision transformer, exascale foundation model, high performance computing, energy efficiency","Dr. Xiao Wang is a research staff scientist in the Computational Science and Engineering Division at Oak Ridge National Laboratory (ORNL). He earned dual Bachelor's degrees in Mathematics and Computer Science from Saint John’s University, MN (2012), and completed his M.S. and Ph.D. in Electrical and Computer Engineering at Purdue University (2016–2017) under Dr. Charles Bouman and Dr. Samuel Midkiff. Before joining ORNL in 2021, he conducted postdoctoral research at Harvard Medical School and Boston Children’s Hospital, focusing on medical imaging. Dr. Wang’s research lies at the intersection of artificial intelligence (AI), high-performance computing (HPC), and computational imaging. He develops algorithms that integrate AI, imaging physics, and HPC to enable high-resolution, data-efficient imaging across modalities such as X-ray, CT, MRI, electron tomography, and satellite imaging, with applications in medicine, biology, climate science, and national security. He received the 2022 AAPM Truth CT Reconstruction Challenge award, was a finalist for the ACM Gordon Bell Prize in 2017 and 2024, and received the 2024 HPCWire Top Supercomputing Achievement Award. His current work focuses on scalable, energy-efficient, and trustworthy Vision Transformer foundation models for large-scale imaging applications."
proj113s1,Accept Wave 2 (Confirmed),Yang,Liu,y-liu@tamu.edu,Texas A&M University,Department of Nuclear Engineering,,,LLM-Enhanced Cyber-Physical Testbed for Advanced Reactor Monitoring and Predictive Maintenance,LLM-Enhanced Cyber-Physical Testbed for Advanced Reactor Monitoring and Predictive Maintenance,,Artificial Intelligence and Intelligent Systems; Mechanical Engineering; Other Engineering and Technologies,"This project aims to harness cutting-edge large language models (LLMs) to enhance real-time monitoring, anomaly detection, and predictive maintenance in a thermal-fluid facility designed for advanced nuclear reactors. By integrating sensor data, experimental logs, and domain-specific knowledge, the LLM-based platform will identify off-normal events and potential cybersecurity threats, providing timely diagnostics to operators. The resulting methods and tools will be published openly, enabling broader adoption within the nuclear industry and fostering safer, more efficient reactor operations.","Generative AI, Large language model, Retrieval-Augmented-Generation, AI Agents, Context engineering.",,Domain Knowledge-Enhanced Generative AI for Advanced Energy Research and Development,Generative AI; Advanced Energy Systems; Automating Engineering Workflow; Real-Time Remote Monitoring and Control; Modeling and Simulation; Retrieval Augmented Generation,"Dr. Yang Liu is an Assistant Professor of Nuclear Engineering at Texas A&M University. He leads the Scientific Machine learning for Advanced Reactor Technologies (SMART) lab and serves as the director of the Generative AI for Science and Engineering (GAISE) Lab under Texas A&M Institute of Data Science. Prior to joining TAMU, he held positions as staff at Argonne and postdoc at University of Michigan. His research focuses on the intersection between AI/ML and advanced energy systems, including (a). Physics-informed machine learning; (b). Digital twin-enhanced experimentation; and (c). Generative AI for science and engineering. Dr. Liu has published more than 50 papers in refereed journals and conference proceedings. He is a recipient of US. Department of Energy Nuclear Energy Office's Distinguished Early Career Award in 2024."
proj117s1,Accept Wave 2 (Confirmed),Anima,Anandkumar,anima@caltech.edu,Caltech,Computing and Mathematical Sciences,,https://tensorlab.cms.caltech.edu/,Neural Operators for Scalable and Sustainable Scientific Modeling,Aligning AI models for scientific simulations under a physics-informed framework,,Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; Computer Science; Other Computer and Information Sciences,"Addressing global sustainability challenges requires fast, accurate, and generalizable modeling of complex systems in energy, climate, and infrastructure. Traditional high-fidelity simulations are often too slow and computationally expensive for timely exploration, design, and decision-making. This project leverages and advances neural operator frameworks to accelerate scientific simulations while adhering to known physical laws. By combining the expressiveness and inference speed of deep learning architectures with physics knowledge, these neural operators provide predictive surrogates for systems governed by partial differential equations, enabling scalable, energy-efficient computation at previously inaccessible scales. In this research, embedding physical constraints ensure sreliable predictions, generalizes modeling across diverse domains, and enables inverse design to identify system configurations that meet performance goals. Uncertainty quantification and formal verification in Lean (a theorem prover) provide further guarantees of correctness and reliability. Physics-informed enhancements, operator-based multi-scale learning, and robust modeling strategies ensure accurate long-term behavior and broad applicability to complex real-world systems. These tools support high-impact simulations for renewable energy, environmental resilience, and critical infrastructure planning. By releasing open-source frameworks, fostering accessibility, and prioritizing usability, this work empowers scientists, engineers, and students worldwide to harness advanced neural operators for transformative discovery, informed decision-making, and sustainable, verifiable solutions to urgent global challenges.","<ul><li>Knowledge of machine learning fundamentals</li><li>Strong programming skills and experience with scientific computing and deep learning libraries (e.g., Python, PyTorch)</li><li>Familiarity with differential equations and numerical methods</li><li>Interest in computational modeling and applying AI to scientific problems</li><li>Enthusiastic and proactive in exploring new research questions, methods, and learning opportunities</li><li>Open-minded and adaptable, eager to engage with diverse scientific approaches and perspectives</li><li>Passion for interdisciplinary research bridging AI and physical sciences</li><li>Effective communication skills</li><li>(Optional) Familiarity with theorem provers such as Lean or an interest in learning them can be helpful for certain specific directions, though not required for most directions</li></ul>",,Neural Operators for Scalable and Sustainable Scientific Modeling,AI for science; neural operators; physics-informed ML; accelerating simulations and scientific discovery; inverse design;,"Anima has made fundamental contributions to AI that is revolutionizing scientific modeling and discovery. She invented Neural Operators for learning multiscale phenomena that frequently occur in nature, such as fluid dynamics, material modeling and wave propagation. She employed Neural Operators to train the first AI-based high-resolution weather model, tens of thousands of times faster than existing physics-based forecasting. Her AI algorithms have enabled many other scientific advances such as modeling plasma evolution in nuclear fusion, enabling safer autonomous drone flights, and designing novel medical devices, drugs, and functional enzymes. Earlier in her career, Anima spearheaded the development of tensor methods, probabilistic latent variable models, and analysis of non-convex optimization. Anima is Bren Professor at Caltech. She previously was a Senior Director of AI Research at NVIDIA and Principal Scientist at Amazon Web Services. She received her B.Tech from IIT Madras, and her Ph.D. from Cornell University. She did her postdoctoral research at MIT and an assistant professorship at UC Irvine. She has received several honors such as the IEEE fellowship, Alfred. P. Sloan Fellowship, NSF Career Award, and Faculty Fellowships from Microsoft, Google, Facebook, and Adobe. She is part of the World Economic Forum's Expert Network."
proj119s1,Accept (Confirmed),Megan,Phinney,mphinney@lanl.gov,Los Alamos National Laboratory (LANL),,,,Super Containers at Super Scale,,Charliecloud,High Performance Computing,"Your job will be software development on Charliecloud: programming; writing and editing documentation; and analyzing bug reports and feature requests. You will participate in the broader project, such as internal/external collaborations, strategy planning, code review, and research. You will collaborate with colleagues inside and outside LANL, both formally and informally, in both written and oral contexts. Finally, you will participate in technical events (e.g., lectures) and professional development activities across LANL. This is a mentored role, and your success will be a top priority for your mentor. Expect to see them on a daily basis for detailed technical and professional guidance and be fully on-site to facilitate better collaboration in this junior role.","An understanding how the command line works, Linux/Unix, Shell commands, Git, Python, C, and POSIX sh programming",,Super Charliecloud Containers at Super Scale,Containers; Linux/Unix Shell commands; Git; Python; C; POSIX sh programming,"Megan earned a B.S. in computer engineering at Iowa State University in 2022. She joined Los Alamos National Laboratory in 2020 as an undergrad student and was promoted to staff scientist in 2022. At LANL, she works on container runtimes and workflows as a part of the Charliecloud project. She also provides user support for HPC systems. In her free time, she enjoys reading, hiking, and playing with her pit bull."
proj123s1,Accept Wave 2 (Confirmed),Edgar,Lobaton,edgar.lobaton@ncsu.edu,North Carolina State University,,,,AI-Guided Learning in JupyterHub Environments,Providing GPU Resources for Deep Learning at NC State,,"Applied Computer Science; Educational Sciences; Electrical, Electronic, and Information Engineering","Ready to build the future of coding education? This high-impact research project invites motivated students to integrate Large Language Models (LLMs) into the JupyterHub platform for technical coursework, with the core goal of transforming the LLM from a passive tutor into an active, reflective guide. You'll be responsible for developing and deploying the LLM interface within Jupyter Notebooks, implementing a fine-grained tracking and logging mechanism to capture student-LLM interaction data, and creating prompt engineering strategies that strictly enforce the LLM's role as a ""navigator"", while offering directional guidance, samples and logic critiques rather than direct solutions. This work lies at the critical intersection of AI, Human-Computer Interaction, and Educational Technology, offering a unique opportunity to directly influence how the next generation of technical professionals learns to code by leveraging AI as a powerful learning partner.","- Proficiency in Python programming - Familiarity with the use of the API for any LLM (e.g., OpenAI API) - [Desired but not required] Familiarity with Kubernetes","As part of this project, you will also help with expanding on the features of our existing JupyterHub infrastructure through JetStream2.","LLMs as Co-Pilots in Scientific Modeling, Coding and Learning",Code assistant; LLMs; AI as a guide; Physics Modeling,"Edgar J. Lobaton is a Professor in the Department of Electrical and Computer Engineering (ECE) at North Carolina State University (NCSU). He joined the department in 2011. Lobaton earned his B.S. in Mathematics and Electrical engineering from Seattle University in 2004. He completed his Ph.D. in Electrical Engineering and Computer Sciences from the University of California, Berkeley in 2009. Lobaton was engaged in research at Alcatel-Lucent Bell Labs in 2005 and 2009. He was awarded the NSF CAREER Award in 2016. He was also awarded the 2009 Computer Innovation Fellows post-doctoral fellowship and conducted research in the Department of Computer Science at the University of North Carolina (UNC) at Chapel Hill from 2009 until 2011. In 2023, he received the William F. Lane Outstanding Teaching from the ECE Department. In 2024, he received the University Faculty Scholars and the Outstanding Teacher Awards from NC State. His research focuses on the integration of AI, and physical and probabilistic modeling applied to cyber-physical systems in areas such as wearable health monitoring, rehabilitation robotics, agriculture and biological imaging."
proj125s1,Accept (Confirmed),Kenneth,Moreland,morelandkd@ornl.gov,Oak Ridge National Laboratory (ORNL),,,https://www.kennethmoreland.com/,Advanced Scientific Visualization with Viskores,,Viskores,Computer Science; High Performance Computing; Open Source Software; Software Engineering; Visualization and Human-Computer Systems,"This project involves the Viskores software library (https://github.com/Viskores/viskores). This library provides functionality to process data that, most often, is generated by physics simulations and provides visual representations to improve data understanding. The Viskores algorithm performs numerous types of computational geometry to extract visually meaning features from data such as contour surfaces from fields and tracing paths within flow. Viskores is also responsible for the rendering of such features using a variety of computer graphics techniques. The specifics of the project will be tailored to the intern’s interests and abilities. The primary needs of the project involve an expansion of Viskores rendering capabilities, optimization of Viskores algorithms, and improved Viskores documentation.","C++; Familiarity with entering commands in a command prompt (e.g, terminal, xterm, powershell, etc.)",,Visualization at Exascale with Viskores,visualization; HPC; GPU,"Dr. Kenneth Moreland is a senior research scientist at Oak Ridge National Laboratory. He received BS degrees in computer science and in electrical engineering from the New Mexico Institute of Mining and Technology in 1997. He received MS and Ph.D. degrees in computer science from the University of New Mexico in 2000 and 2004, respectively. Dr. Moreland specializes in large-scale visualization and graphics and plays an active role in the development of several HPC products including Viskores, ParaView, VTK, IceT, and Catalyst. His current interests include the design and development of visualization algorithms and systems to run on multi-core, many-core, and future-generation computer hardware."
proj126s1,Accept (Confirmed),Damien,Lebrun-Grandie,lebrungrandt@ornl.gov,ORNL,,,https://www.ornl.gov/staff-profile/damien-lebrun-grandie,Kokkos Tools,,Kokkos,Applied Computer Science; High Performance Computing; Performance Evaluation and Benchmarking; Software Engineering,"This internship focuses on developing Kokkos Tools, a vital suite of libraries for analyzing and optimizing Kokkos applications without changing the source code. You will help expand the Kokkos Tools ecosystem by building: Performance & Energy Profiling tools to measure execution time, memory usage, and energy consumption. This work helps developers identify bottlenecks and optimize code for diverse hardware, including CPUs and GPUs. Code Sanity & Debugging tools to detect and diagnose common programming errors and invalid usage of the Kokkos API, ensuring code correctness. You will contribute to a major open-source project used globally in High-Performance Computing (HPC). You'll gain hands-on experience in parallel programming, performance-portability, and the full software development lifecycle. This is a chance to build critical skills in performance analysis and debugging while connecting with the international HPC community.","We're looking for candidates with a passion for High-Performance Computing (HPC) and parallel programming. Relevant backgrounds could include Computer Science, Engineering, or a related technical field. Ideal interests and skills include: Experience with C++ (or a similar high-performance language). Familiarity with parallel programming models (e.g., CUDA, OpenMP, MPI, or Kokkos). An interest in performance analysis and finding ways to optimize code. A desire to contribute to a large-scale open-source project.",,Your Summer Project: Taming the World's Fastest GPUs,Performance Portability; Exascale Computing; C++; Heterogeneous Architectures; GPU; Parallel Programming; Tooling / Profiling,"Damien Lebrun-Grandié is a Senior Computational Scientist at Oak Ridge National Laboratory with over a decade of experience in the field. He holds a PhD in Nuclear Engineering from Texas A&M University, a MSc in Physics from the Karlsruhe Institute of Technology in Germany, and a MEng in Physics Engineering from Grenoble INP in France. His research focuses on developing algorithms and enabling technologies for solving large-scale, complex engineering, and scientific problems. As a founding member of the High Performance Software Foundation, Damien was instrumental in getting the organization started and continues to play a leading role on the Governing Board, representing the Technical Advisory Council. He is also the co-lead of the Kokkos C++ performance portability project, where he oversees a large international team of developers and researchers. Additionally, he represents ORNL on the C++ Standards Committee, where he has been a key contributor to foundational features for scientific computing like std::mdspan in C++23 and std::linalg for C++26."
proj128s1,Accept (Confirmed),Todd,Gamblin,tgamblin@llnl.gov,LLNL,Livermore Computing,jpg Information Type: jpgSize: 196KBUploaded: Oct 03MD5: 0e975e2d14afbe32e881a7785ba0a61cOriginal Name: me-greatwall-fron... view move to AWS,,Spack at LLNL,,Spack,Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; Computer Science; High Performance Computing; Open Source Software; Software Engineering,"Depending on experience and interest, we will develop a project dealing with the open source Spack package manager (https://github.com/spack/spack) or the Spack package ecosystem (https://github.com/spack/spack-packages). Spack has become the de-facto package management tool for High Performance Computing, an area where software complexity is constantly rising. Spack helps users build, install, and manage software on laptops, desktops, and supercomputers. With over 1,500 contributors and over 8,500 package recipes, Spak provides a valuable resource for HPC users around the world. Example projects would include new core features for Spack, including terminal UI improvements, performance improvements, or enhancements to Spack's solver or other capabilities. Projects could also focus on package recipe improvements, e.g. improvements to AI packages or GPU support, improvements to Spack's continuous integration system. We can tailor the project to the applicant's experience and interest. On this project, you’ll learn more about the core of Spack, how it builds software, and how large software ecosystems are sustained through community effort.","For core Spack projects, we expect experience with Python and some Linux systems programming. An ideal candidate will have at least some experience with building and installing software, especially with build systems like CMake and Autotools. Low-level systems experience, e.g. UNIX process control, file I/O, and performance profiling are a plus. Experience with performance optimization and with using package managers, Spack or otherwise, are a plus.",,,,
proj129s1,Accept (Confirmed),Andrew,Myers,atmyers@lbl.gov,LBNL,Applied Mathematics,,https://profiles.lbl.gov/22224-andrew-myers,Python-driven workflows with AMReX,,AMReX,Applied Computer Science; Applied Mathematics; High Performance Computing; Software Engineering,"AMReX is a publicly available software framework designed for building massively parallel block- structured adaptive mesh refinement (AMR) applications. Simulation codes based on AMReX model a wide range phenomena from fields ranging from astrophysics and cosmology to plasma physics, earth systems modeling, multi-phase flow, epidemiology, cell biology, and more. While AMReX is written in C++, for this internship we are envisioning several projects that involve improving and expanding the Python interfaces to AMReX with the goal of supporting new AI/ML use cases, including: 1. Integrating simulations with ML-based Bayesian optimization workflows 2. Training fast surrogate models and incorporating those into simulations 3. Automatic differentiation of coupled C++ simulation and Python analysis code through tools like Enzyme. 4. Uncertainty quantification using frameworks like PyTUQ. The above workflows will be demonstrated on and evaluated with real-world AMReX-based application codes.","Experience with Python is desired, other useful skills include experience with C++, AI/ML, and high-performance computing.",,AMReX: Adaptive Mesh Refinement for Exascale,Adaptive Mesh Refinement; Math libraries; Scientific Computing; C++; Python; AI/ML; automatic differentiation; uncertainty quantification,"Andrew joined Berkeley Lab in 2013 as a postdoctoral researcher in the Applied Numerical Algorithms Group. He is now a staff member in the Center for Computational Sciences and Engineering, where he designs and implements HPC algorithms for solving multiscale, multiphysics problems using structured adaptive meshes and particles. He is a core contributor to the AMReX adaptive mesh library, and also contributes to a number of AMReX-based simulation codes in subjects ranging from computational plasma physics to epidemiology. He was a member of the 2022 Gordon-Bell prize-winning team for kinetic plasma simulations with the WarpX Particle-in-Cell code, and received a Director's Award for Exceptional Scientific Achievement in 2023."
proj134s1,Accept Wave 2 (Confirmed),Varun,Kasireddy,vkasired@alumni.cmu.edu,Carnegie Mellon University,Robotics,,https://teamchiron.ai,Visual Language Model for Stand-off Triage Sensing,LLM Training and Evaluation for Pandemic Prevention and Response,,Computer Science,"We build vision-language systems that help robots perform fast, reliable casualty triage. As our work involves practical edge deployment (e.g., NVIDIA Jetson), a critical aspect in our decision making is to balance algorithm accuracy with latency requirements. By Summer 2026, the project will be at a point where students will test and iterate on existing pipelines—video highlight extraction, robust person and blood segmentation, and Visual Question Answering (VQA) for scene understanding. Each week, we will run systems tests and score performance. Based on the results, students should quickly synthesize progress, create short presentations for the broader team, and help the project leads finetune models and manage versioning to push improvements for the next deployment. Work spans dataset preparation, annotation, benchmarking, and optimization on GPU clusters. The aim is clear tools that fit robotic workflows. Impact includes emergency response, safety monitoring, and human-robot teaming. It’s hands-on and fast-paced—you’ll see changes week to week and get to validate them on real hardware.","• Core: Python, basic Linux, Docker containerization, and comfort with Git/GitHub for fast iteration. • Robotics: ROS 2 basics (nodes, topics, launch files) to run weekly systems tests and integrate updated components. • Experiment tracking: Weights & Biases (W&B) for logging metrics, comparing runs, and sharing dashboards with the team. • Evaluation: Clear thinking about test scores, precision/recall, latency, and failure cases; ability to propose quick fixes. • Data pipelines: PyTorch, computer vision, segmentation, dataset curation/annotation (e.g., CVAT/FiftyOne), and simple GPU optimization. • Optional: Edge deployment (e.g., NVIDIA Jetson)",,Push AI-driven robots to the field,Robotics; Multimodal Perception; Vision‑Language Models; Edge Computing; ROS 2; Real‑Time Evaluation; Reliable AI; Field Testing; Sensor Fusion; LiDAR,"Varun Kasireddy is a Project Scientist at Carnegie Mellon University’s AirLab. His research focuses on multimodal perception, vision–language models, and learning on edge devices. He also collaborates with academic and industry partners, including efforts on autonomous aerial systems for object identification and tracking. Varun is committed to reproducible research, practical evaluation, and human‑in‑the‑loop development. He mentors students on concise experimentation, observability, and data quality, and enjoys designing small-scale prototypes that translate quickly to field trials. Through SRP, he aims to build a collaborative summer project that combines rigorous evaluation with scalable perception to deliver reliable robotic behaviors in real environments."
proj136s1,Accept Wave 2 (Confirmed),Agniv,Sengupta,agsengupta@ucsd.edu,University of California San Diego,Scripps Institution of Oceanography,jpg Information Type: jpgSize: 3MBUploaded: Oct 08MD5: f66aa5f053cad335f8215df3d1ee49f6Original Name: Agniv_Sengupta_ph... view move to AWS,https://agsengupta.scrippsprofiles.ucsd.edu/,AI Models for the Prediction of Extreme Weather Events,Development of AI Data-driven Models and Very Large Ensembles for the Prediction of Atmospheric Rivers and Extreme Weather Events,,Artificial Intelligence and Intelligent Systems; Atmospheric Sciences,"Accurate weather forecasting has traditionally relied on numerical weather prediction models, which require significant computational resources. In this context, artificial intelligence (AI) models have revolutionized the domain of weather prediction in the past 2-3 years, emerging as computationally efficient alternatives. As part of our NAIRR Pilot project, we are developing such AI weather modeling and prediction capabilities over the North Pacific and the western United States, specifically for extreme weather phenomena such as atmospheric rivers (ARs). In this work, we utilize state-of-the-art AI advancements, including graph neural networks and Diffusion-based Generative AI methods. Additionally, we employ these models to generate a large number of realizations of future weather, enhancing the tracking of AR storms from their genesis over the Pacific to their impact over the U.S. West Coast.","Interest in AI/ML, Meteorology, or Climate Science Experience with, or a desire to learn: * Python * Statistics * Data Analysis * Data Visualization * Machine Learning tools (e.g., PyTorch, TensorFlow, Keras, Scikit-learn) Coursework or experience in Statistics or Data Science is preferred. Familiarity with Machine Learning is a plus.",Opportunity to be co-hosted alongside a cohort of interns selected through the Scripps Institution of Oceanography (SIO)/CW3E Summer Internship Program at the beautiful SIO campus overlooking the Pacific Ocean.,AI Data-driven Models for the Prediction of Extreme Weather Events,Weather; Meteorology; Artificial Intelligence; Machine Learning,"Dr. Agniv Sengupta is a Sr. Computational Research Scientist and the Machine Learning Team Lead of the Center for Western Weather and Water Extremes (CW3E), Scripps Institution of Oceanography at UC San Diego. His research interests involve the prediction of high-impact weather events using artificial intelligence and machine learning. His current projects focus on improving the prediction skill of weather (0-10 days), subseasonal (1-6 weeks), and seasonal (1 to 6 months) forecasts in the Western United States. This involves exploring innovative algorithms and approaches, advancing models for predictions across multiple timescales, and developing decision-support tools and forecast products in coordination with stakeholders. Dr. Sengupta earned his Ph.D. (2020) and M.S. (2016) in Atmospheric and Oceanic Science from the University of Maryland College Park, and was subsequently a postdoctoral scholar (2020-21) at the NASA Jet Propulsion Laboratory (JPL) prior to joining CW3E."
proj137s1,Accept Wave 2 (Confirmed),Reno,Kriz,rkriz1@jhu.edu,Johns Hopkins University,Human Language Technology Center of Excellence,jpg Information Type: jpgSize: 2MBUploaded: Oct 08MD5: e62d9f661ef32b6fba6ad6916823825eOriginal Name: reno_headshot.jpeg view move to AWS,https://hltcoe.jhu.edu/researcher/reno-kriz/,SCALE 2026: Event Understanding and Summarization from Real-time Videos,Advancing Scientific Discovery through Multilingual/Multimodal Summarization at SCALE 2025/2026,,"Artificial Intelligence and Intelligent Systems; Computer Science; Electrical, Electronic, and Information Engineering; Informatics, Analytics and Information Science","understand real-time, multilingual video content is increasingly important. From smartphone footage of natural disasters to public livestreams near high-risk infrastructure, these unedited clips offer firsthand evidence of unfolding events. Combined with audio and embedded text, they form a rich multimodal source that remains underutilized in current retrieval-augmented generation systems. Especially for real-time situations, scientific advances grounding articles in video can combat misinformation and help journalists quickly synthesize information from non-traditional, cross-lingual platforms. SCALE 2026, a 10-week workshop hosted by the Human Language Technology Center of Excellence (HLTCOE) at Johns Hopkins University, provides a realistic setting for advancing real-world multimodal understanding. During the summer, we will evaluate modality-specific technologies for extracting relevant signals from raw video data. First-stage research areas include audio and visual event detection, speech and audio summarization, and OCR or visual frame analysis. These signals will inform the second stage, our primary task of multimodal retrieval-augmented generation. Given an information need and a collection of raw multilingual videos, the system must retrieve relevant content and generate a coherent summary of the most significant information. Second-stage research areas include multimodal information retrieval and multi-video summarization.","We welcome participants with backgrounds or interests in natural language processing, computer vision, and multimodal technologies. Relevant experience might include work with large language or vision-language models, video or audio understanding, and multilingual or low-resource technologies. Participants interested in areas such as event detection, speech processing, optical character recognition, information retrieval, or summarization are especially encouraged to apply. SCALE projects are highly collaborative, bringing together researchers from diverse areas of expertise, so openness to interdisciplinary teamwork and shared problem-solving is essential.","SCALE (the Summer Camp for Applied Language Exploration) is an annual 10-week research program hosted by the HLTCOE at Johns Hopkins University since 2009. For more on its history and past topics, see https://hltcoe.jhu.edu/research/scale/. SCALE 2026 builds on the two most recent workshops: SCALE 2024, focused on event-centric video retrieval, and SCALE 2025, which explored retrieval-augmented generation for request-guided summarization of multilingual sources.",SCALE 2026: Event Understanding and Summarization from Real-time Videos,multimodal retrieval-augmented generation; multi-video summarization; video retrieval; speech/audio summarization; optical character recognition; audio/visual event detection; computer vision; speech processing,"Reno Kriz is a research scientist at the Johns Hopkins University Human Language Technology Center of Excellence (HLTCOE). His primary research interests involve leverage large pre-trained models for a variety of natural language understanding tasks, including those crossing into other modalities, e.g., vision and speech understanding. These multimodal interests have recently involved the 2024 and 2026 Summer Camps for Language Exploration (SCALE) on event-centric video retrieval, understanding, and summarization. He received his PhD from the University of Pennsylvania where he worked with Chris Callison-Burch and Marianna Apidianaki on text simplification and natural language generation. Prior to that, he received BA degrees in Computer Science, Mathematics, and Economics from Vassar College."
proj140s1,Accept Wave 2 (Confirmed),Shi Zhuo,Looi,looi@caltech.edu,California institute of technology,"Mathematics (Division of physics, mathematics and astronomy)",jpg Information Type: jpgSize: 114KBUploaded: Oct 09MD5: 38cdb0a0e0b33ef6becea12d2c884eb2Original Name: unnamed (1).jpg view move to AWS,https://sites.google.com/view/s-looi/,AI-Driven Methods for Discovering and Proving Mathematical Inequalities,AI-Driven Methods for Discovering and Proving Mathematical Inequalities,,Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; Computer Science; High Performance Computing; Performance Evaluation and Benchmarking; Training,"Our proposed work advances AI for Accelerating Science and Discovery, a core focus area of the NAIRR Pilot, by developing a specialized, AI-driven framework for discovering and proving mathematical inequalities. Inequalities are foundational tools across scientific and engineering domains, from verifying stability in partial differential equations to bounding error terms in applied mathematics and physics. By combining large language models, reinforcement learning (RL), and symbolic computation, we aim to automate and accelerate the process of finding new results in both classical and cutting-edge mathematical research. This will facilitate the proof of inequalities and has the potential to enable new discoveries in science and engineering where precise mathematical bounds are important.",Experience training models and/or handling datasets for language models,,AI-Driven Discovery and Proof of Mathematical Inequalities,,"Looi works in mathematical analysis, evolutionary PDEs, and AI. A core part of his research in analysis and PDEs involves the study of inequalities, including functional and polynomial inequalities, which are intimately related to many areas of pure and applied mathematics. In the AI-math domain, his contributions include proving theorems on the controllability of self-attention in “What’s the Magic Word?”, leading the data team at Project Numina (winner of the 2024 AIMO Prize), and advancing Lean-4 formalization in PDEs. He is a founding scientific advisor to ScienceStack, a platform for interactive, machine-readable papers. His work aims to merge machine learning, formal methods, and math to build reliable and reproducible reasoning workflows in mathematics."
proj141s1,Accept Wave 2 (Confirmed),Zhe,Zhang,zhezhang@tamu.edu,Texas A&M University,Geography,jpg Information Type: jpgSize: 484KBUploaded: Oct 09MD5: 2b76038cf01c04c1ac1e9272dfa76666Original Name: zhesarina-zhang.jpg view move to AWS,,Utilizing NAIRR Pilot Resources for Building Sustainable Blue Economy,Utilizing NAIRR Pilot Resources for Building Sustainable Blue Economy,,Educational Sciences; Environmental Biology; Environmental Biotechnology; Environmental Engineering; Other Earth and Environmental Sciences,"The Gulf of America faces significant challenges for Blue Economy research, including environmental degradation from pollution and overfishing, as well as the impacts of natural disasters. These issues strain marine ecosystems and coastal communities, creating barriers to sustainability. Similarly, the mid-Atlantic region, such as Maryland, suffers from agriculture, urban runoff, and industrial activities that degrade water quality and significantly impact fish populations and the overall health of the fishery. Hawaii, meanwhile, encounters unique challenges due to its geographic isolation and heavy reliance on ocean resources. This project aims to address these challenges by establishing coordinated efforts to improve Blue Economy research in these regions, enhancing funding for innovative research, and strengthening partnerships across sectors to build a Blue Economy research network. This project will bring together students, faculty, and researchers from U.S. institutions, along with participants from broad fields, including GIScience, oceanography, computer science, biology, and civil engineering. The goal is to promote AI education and workforce development in Blue Economy research. Nowadays, AI has transformative potential to enhance blue economy research by providing advanced tools for data analysis, modeling, and decision-making.","The workshop will offer training opportunities to help research communities effectively navigate NAIRR pilot resources, including learning how to access and use these resources for ocean and coastal sustainability analysis. All workshop and training materials will be made publicly available through a project GitHub repository and website, ensuring broad accessibility and ongoing learning opportunities. Participants will learn skills of cyberinfrastructure and high-performance computing, oceanography, disaster management, and coastal resilience planning.",,Utilizing NAIRR Pilot Resources for Building Sustainable Blue Economy,"NAIRR, Cyberinfrastructure, Blue Economy, Artificial Intelligence, Oceanography, GIScience","Dr. Zhe Zhang is an Associate Professor in the Department of Geography at Texas A&M University. Dr. Zhang has served as Chair of the Cyberinfrastructure Specialty Group of the American Association of Geographers and was elected to the Board of Directors of the Cartography and Geographic Information Society. She also serves as Chair of the Research Committee of the University Consortium for Geographic Information Science. Her research focuses on developing spatial decision support systems by integrating advanced cyberinfrastructure, geospatial artificial intelligence, and participatory design to address critical challenges in disaster management and sustainability. Dr. Zhang serves as the Co-Principal Investigator of the Texas A&M FASTER High-Performance Supercomputer (over $3 million) and as a Co-Investigator of the Texas A&M ACES Supercomputer (over $12 million), both supported by the NSF. In addition, she serves as Principal Investigator on eight externally funded grants from (NSF, NASA, USDOT, NOAA, and National Geographic Society), totaling over $3 million. Dr. Zhang has been honored to receive both the Pathways Award from Texas A&M Faculty Affairs and the National Science Foundation CAREER Award in recognition of her impactful research."
proj143s1,Accept Wave 2 (Confirmed),Avi,Sahu,asahu@salud.unm.edu,UNM Comprehensive Cancer Center,Dept of Internal Medicine,png Information Type: pngSize: 1MBUploaded: Oct 10MD5: baa1c5fc89eaf1d6812fa40777e88ce6Original Name: headshot.png view move to AWS,https://www.tumorai.org,Bridging Molecules and Medicine: Explainable AI for Genetics and Imaging,Advancing Colorectal Cancer Diagnosis: A Multimodal AI Copilot for Real-Time Endoscopy,,Artificial Intelligence and Intelligent Systems; Biochemistry and Molecular Biology; Clinical Medicine; Health Sciences,"Millions of people face deep uncertainty in their fight against cancer. Two major hurdles often stand in the way of clear answers. First, genetic testing frequently returns ambiguous results called ""variants of unknown significance"" (VUS), leaving nearly 40% of patients in an anxious ""diagnostic limbo"" without clear medical guidance. Second, a nationwide shortage of specialists means that interpreting crucial medical images, like endoscopies, can be slow and inconsistent, delaying life-saving diagnoses. 🩺 Our lab is pioneering a solution by building AI ""co-pilots"" for medicine. We have already developed the core technology: powerful AI models that can translate complex genetic data (Protein2Text) and interpret endoscopic images (EndoPilot). This project will focus on the most exciting application of these powerful tools. We will create Mutation2Text to demystify VUS with clear, understandable rationales. Concurrently, we will build MED-X, a framework where a team of our AIs work together (AI agents) to help doctors spot signs of colorectal cancer with greater accuracy. This project will deliver a unified pipeline that empowers clinicians, provides clear answers to patients, and makes expert-level care accessible to all.","We are building a diverse team and welcome collaborators from all backgrounds. Whether you are a coder, biologist, future doctor, or creative problem-solver, there is a place for you. We seek faculty/students with interests in: Technology & Data Science: Help build and train cutting-edge AI models using skills like Python and machine learning. Biology & Health Sciences: Use your background in genetics or life sciences to guide our AI’s reasoning and ensure its insights are clinically useful. Human-Centered Thinking: Help us create trustworthy, explainable AI that doctors can confidently use to improve patient care.",,Explainable AI Co‑Pilots for Genes and Endoscopy,Explainable AI; Genomics; Variant interpretation; Endoscopy; Multimodal LLMs; Human‑AI collaboration; Cancer prevention; Precision oncology; Ovarian cancer; Colorectal cancer.,"Avinash (“Avi”) Das Sahu, PhD is an assistant professor at UNM Comprehensive Cancer Center and Dept of Computer Science, where he leads th TumorAI lab (Tumorai.org). His group designs interpretable, clinician ready AI that links molecular signals to real world decisions in cancer care. The group builds AI “co pilots” across two fronts: (1) genomics, including Protein2Text and the forthcoming Mutation2Text to explain variants—especially VUS—with clear, rationale based text; and (2) medical imaging, where EndoPilot and the multi agent MED X framework support transparent, human in the loop analysis of endoscopic images for colorectal cancer. Guided by a simple aim—to “prevent the preventable and treat the treatable”— he collaborates closely with oncologists and geneticists to move these tools from algorithms to clinic ready prototypes. His work has been recognized with awards such as the OCRA CDRG (PI), NIH Pathway to Independence (K99/R00), and the Michelson Prize, following training at the University of Maryland/NHGRI and postdoctoral work at Dana Farber/Harvard and the Broad Institute. Beyond publishing in venues such as Cancer Discovery and Nature Communications, his lab emphasizes open science, inclusive mentoring, and deployment on high performance computing resources to broaden access to expert level decision."
proj144s1,Accept Wave 2 (Confirmed),Tuan,Do,tdo@astro.ucla.edu,UCLA,,jpg Information Type: jpgSize: 412KBUploaded: Oct 10MD5: 2ccc4331181e6d01ec92e4f2e1cc8b52Original Name: Tuan_Do_P8300064r... view move to AWS,https://datalab.astro.ucla.edu/,Integrating LLMs into Machine Learning for Physics and Astronomy Education,Improving Access to Computation for Machine Learning for Physical Sciences Course,,Artificial Intelligence and Intelligent Systems; Astronomy and Planetary Sciences; Particle and High-Energy Physics,"This project is to study how to use large language models (LLMs) and related technologies into a course on Machine Learning for Physical Sciences at the upper division undergraduate level. LLMs are now a prominent technology in computer science and the industry, and many students have experience using them. However, students do not typically get the opportunity to setup their own LLM and do experiments on them. Specifically, the use of LLMs in scientific education is not well explored. This project aims to develop a course unit on both using existing LLMs as well as the underlying LLM architectures (e.g. transformers) for science. Potential projects include setting up and deploying local LLMs and examining their abilities to do physics research. Another is to combine images and text from astrophysics into a transformer to use data fusion for classification. By giving students experience in underlying technologies behind these tools, they will become more knowledgeable and responsible users of AI and develop skills useful for their careers.","Background in teaching, designing lab courses, and/or deploying large language models.",,Developing an LLM curriculum for AI/ML in Physical Sciences courses,LLMs; transformers; science education; Physical Sciences; Physics; Astrophysics; Data Science; Lab Classes,Tuan Do is an Associate Professor at UCLA in the Physics and Astronomy Department. He is the PI of the UCLA Astrophysics Data Lab. He got his undergraduate degrees in Physics and Astrophysics from UC Berkeley and his Astrophysics Ph.D. at UCLA. His research focuses on translating ML/AI models for Astrophysics applications. He created the first Machine Learning for Physical Science course at UCLA.
proj146s1,Accept Wave 2 (Confirmed),Haohan,Wang,haohanw@illinois.edu,University of Illinois Urbana Champaign,School of Information Sciences,jpg Information Type: jpgSize: 2MBUploaded: Oct 10MD5: bcb77a700a959547b7266fb7e6ca282fOriginal Name: haohanwang.jpg view move to AWS,https://haohanwang.github.io/,Toward Redefining Disease Taxonomy: Scaling Transcriptomic Analysis with Agentic AI Systems,Toward Redefining Disease Taxonomy: Scaling Transcriptomic Analysis with Agentic AI Systems,,Basic Medicine; Biochemistry and Molecular Biology; Health Sciences,"The project aims to redefine how diseases are classified by using agentic artificial intelligence—a new generation of self-revising, collaborative AI systems—to analyze large-scale human transcriptomic data. Today’s biomedical research relies on rigid pipelines that struggle to integrate data across tissues, populations, and rare conditions. We will build an AI framework composed of autonomous reasoning agents that continuously analyze and validate public datasets, learning to correct themselves and adapt to missing metadata or unexpected patterns. These agents will uncover disease relationships directly from molecular signals rather than from pre-defined diagnostic categories. The outcome is a data-driven taxonomy of disease—linking common and rare conditions through shared regulatory signatures—that could reshape how medicine understands biological variability. Students will join a cross-disciplinary team at the intersection of AI, computational biology, and open science, contributing to a system that learns science itself.","We welcome students from computer science, data science, biology, or related fields who are curious about how AI can advance scientific discovery. Interest in coding, statistics, or genomics is helpful but not required. The most important traits are curiosity, persistence, and comfort working across disciplines. Experience with Python, R, or machine learning frameworks is a plus, but students can learn these skills during the project.","Students will gain experience working with large biological datasets and intelligent systems that operate autonomously. The environment emphasizes mentorship, open collaboration, and publication-quality research. Outstanding participants may continue with the lab through independent study or joint conference submissions.",Toward Redefining Disease Taxonomy,"Disease should not be defined by clinical manifestation, but by the underlying mechanism","Haohan Wang is an assistant professor in the School of Information Sciences at the University of Illinois Urbana-Champaign. His research focuses on the development of trustworthy machine learning methods for computational biology and healthcare applications, such as decoding the genomic language of Alzheimer's disease. In his work, he uses statistical analysis and deep learning methods, with an emphasis on data analysis using methods least influenced by spurious signals. Wang earned his PhD in computer science through the Language Technologies Institute of Carnegie Mellon University where he works with Professor Eric Xing. In 2019, Wang was recognized as the Next Generation in Biomedicine by the Broad Institute of MIT and Harvard because of his contributions in dealing with confounding factors with deep learning."
proj149s1,Accept Wave 2 (Confirmed),Steven,Fernandes,stevenfernandes@creighton.edu,Creighton University,"Computer Science, Design and Journalism",jpg Information Type: jpgSize: 5MBUploaded: Oct 15MD5: 10265fc9952999fb714e9630a89d5355Original Name: Steven_Fernandes.jpg view move to AWS,https://www.creighton.edu/campus-directory/fernandes-steven-l,Building Generative AI Applications,CSC 590 - Building Generative AI Applications,,Artificial Intelligence and Intelligent Systems; Computer Science; Health Sciences; Software Engineering,"This project explores how to build artificial intelligence systems that can create, understand, and assist with real-world tasks. We'll work on three exciting types of applications. First, we'll create an AI that can generate new content, such as images. This has applications everywhere from art and entertainment to marketing and product design. Second, we'll build AI that can search through large collections of information and provide accurate, helpful answers by combining what it finds with its ability to generate responses. Think of this as creating smarter search tools or assistants that can actually comprehend documents and assist people in making informed decisions. Third, we'll develop autonomous AI agents that can complete multi-step tasks independently, like digital assistants that handle scheduling, research, or workflow automation. This work matters because these technologies are transforming every industry from healthcare to education to creative fields. Understanding how to build them responsibly puts you at the forefront of real innovation. We welcome collaborators from all backgrounds and experience levels, whether you're interested in coding, design, ethics, user experience, or simply curious about what AI can do. You'll gain practical hands-on skills while working on applications that could genuinely help people in their everyday lives.","We welcome collaborators from diverse backgrounds and with varying experience levels. Helpful skills include basic programming in any language, an interest in how AI systems work, or experience with data and information. Creative thinking, problem solving, user experience design, and perspectives on ethics or responsible AI development are all valuable. Most importantly, we're looking for curiosity, willingness to learn, and enthusiasm for exploring how these technologies can solve real problems.","If possible, I would be interested in considering the following two students from Creighton University: (1) Our PhD student, Vignesh Rathinavelpandian Anandavel, applied for the student track and was also included in my initial faculty track application. (2) The undergraduate student, Sara Avila, was only included as a student in my initial faculty track application.",Generative AI for Cochlear Hair Cell Detection,Generative AI; Deep Learning; Computer Vision; Machine Learning; Medical Image Processing.,"I am an Assistant Professor of Computer Science at Creighton University, where I have taught and mentored students in computer science, data science, and health informatics since joining in July 2020. My research focuses on developing advanced deep learning models for applications in computer vision, medical imaging, and natural language processing. Before my current role, I completed postdoctoral research in the Department of Computer Science at the University of Central Florida from September 2018 to June 2020, contributing to projects funded by the Defense Advanced Research Projects Agency (DARPA), National Science Foundation (NSF), and Royal Bank of Canada (RBC), with a focus on deep learning and computer vision. Additionally, I conducted postdoctoral research in the Department of Electrical and Computer Engineering at the University of Alabama at Birmingham from July 2017 to August 2018, working on National Institutes of Health (NIH)-funded projects related to deep learning and medical image processing. My research has obtained compute credits through NAIRR Pilot Classroom, NAIRR Startup, Amazon Web Services, and Google Cloud Platform. I have published articles in high-impact AI venues, including NeurIPS, CVPR, ECCV, and ICCV."
proj150s1,Accept (Unconfirmed),Axel,Huebl,axelhuebl@lbl.gov,Lawrence Berkeley National Laboratory,Accelerator Technology & Applied Physics,jpg Information Type: jpgSize: 227KBUploaded: Oct 19MD5: cb9acf1f3bdb4b05b9c432931341da3cOriginal Name: Axel_Huebl.jpg view move to AWS,https://github.com/ax3l/,Novel Exascale & AI Workflows with WarpX,,WarpX,Applied Computer Science; Applied Mathematics; Fluid and Plasma Physics; High Performance Computing; Open Source Software; Particle and High-Energy Physics; Performance Evaluation and Benchmarking; Software Engineering,"We are envisioning several projects that involve improving and expanding the performance or scientific workflows using WarpX, including: 1. Parallel data post-processing with DASK and openPMD. 2. Automatic differentiation of C++ simulations through tools like Enzyme. 3. Implementation of numerical algorithms for the modeling of plasmas, lasers, and particle beams, with applications in fusion and/or accelerator physics. 4. Training fast AI/ML surrogate models. Incorporate those into simulations, integrated research infrastructures, updating models in real-time from experimental and simulation data as they become available, and/or informing operation in experiments. The above implementations and workflows will be demonstrated on and evaluated with WarpX simulations in fusion and particle accelerator physics.","We are looking for people interested in developing new code (C++, Python), implement numerics, or new workflows to address timely challenges in fusion and particle accelerator science. Ideally, you already have experience with C++, Python and Git/GitHub and are eager to be embedded in an open, interdisciplinary team.",,,,
doeproj101s1,Accept (Unconfirmed),Khaled,Ibrahim,kzibrahim@lbl.gov,Lawrence Berkeley National Laboratory,Computer Science,,https://profiles.lbl.gov/21065-khaled-ibrahim,Performant Scientific Workflows in HPC Environments,,,Applied Computer Science; Artificial Intelligence and Intelligent Systems; Computer Science; High Performance Computing; Performance Evaluation and Benchmarking,"Modern scientific workflows are rapidly evolving, integrating LLMs, AI, and simulation codes to control, predict, and model complex discovery processes. Managing these intricate workflows effectively is crucial for leveraging HPC resources, preventing underutilization, and accelerating scientific progress. This project addresses these challenges through two main approaches: 1- Workload Instrumentation to Profile and Model Performance: By instrumenting workflow components, we gather detailed performance metrics to identify bottlenecks, understand resource consumption, and predict performance under various loads, enabling proactive optimization. 2- Orchestration of Resource Scheduling to Improve User Experience: Building on performance insights, we develop sophisticated scheduling mechanisms to dynamically allocate HPC resources, minimizing wait times, maximizing throughput, and enhancing user experience for scientists by accelerating experiment execution and model iteration.","Programming Languages: C++, Python; Profiling infrastructures: Nvidia tools",,Performant Scientific Workflows in HPC Environments,High Performance Computing HPC/AI workflows,"Khaled Ibrahim is a member of the Parallel Performance and AI Nexus (PPAN) group in the Computing Sciences Area. He is working on various research projects on high performance computing focusing on runtime systems, programming models, performance modeling and optimization, and computer architectures. Khaled Ibrahim came to Berkeley Lab in 2009, after working in INRIA, France. He obtained his PhD in computer engineering from North Carolina State University."
doeproj102s1,Accept (Unconfirmed),Andy,Nonaka,AJNonaka@lbl.gov,LBNL,Applied Mathematics,,https://ccse.lbl.gov/index.html,High-Performance Multiphysics and Multiscale Modeling with AMReX,,,"Applied Computer Science; Applied Mathematics; Atmospheric Sciences; Computer Science; Electrical, Electronic, and Information Engineering; Fluid and Plasma Physics; High Performance Computing; Open Source Software; Performance Evaluation and Benchmarking; Software Engineering","The Center for Computational Sciences and Engineering (CCSE) at LBNL is a high-performance modeling and simulation group interested in applications described by complex partial differential equations including fluids (industrial, biological, and micro/nanoscale), plasma physics, microelectronics, and environmental flows. The underlying software framework we develop and use is called AMReX, a GPU/exascale-enabled block-structured adaptive mesh refinement software framework that also includes particle/mesh, embedded boundary, and linear solver capabilities. We also develop data-driven / machine-learning enhancement, and code coupling strategies to both accelerate and increase the fidelity of our codes. We are looking for collaborators with common interests in the computer science, applied mathematics, and/or application science aspects of research performed within CCSE.","C++, python, numerical methods, partial differential equations, domain knowledge in fluids, plasmas, microelectronics, and/or environmental flow.",,High-Performance Multiphysics and Multiscale Modeling with AMReX,"partial differential equations, structured mesh, particle/mesh, data/ML-driven model acceleration, industrial fluids, biofluids, micro/nanoscale fluids, plasmas, microelectronics, environmental flow.","Andy Nonaka is a Staff Scientist and Group Lead of the Center for Computational Sciences and Engineering at Lawrence Berkeley National Laboratory. His research interests include high-performance implementations of multiphysics and multiscale algorithms for partial differential equations, with a recent emphasis on electrodynamical, fluid dynamical, and material science applications. More generally, he is interested in adaptive structured mesh algorithms, particle and particle/mesh methods, linear solvers, and machine learning techniques to accelerate scientific discovery."
doeproj103s1,Accept (Unconfirmed),arianna,formenti,ariannaformenti@lbl.gov,LBNL,,,,Physics simulations and code development for plasma and/or accelerator physics,,,Fluid and Plasma Physics; High Performance Computing; Open Source Software; Particle and High-Energy Physics,"This research project will be hosted by the Accelerator Modeling Program group at Lawrence Berkeley National Laboratory. AMP is a program within the Accelerator Technology and Applied Physics Division at Berkeley Lab that focuses on HPC to model particle accelerators, laser-plasma interactions, beam physics and plasma devices, and nuclear fusion. With this project, you will have the opportunity to contribute to ongoing research and gain hands-on experience in the field of computational physics within an open and team-science driven environment. Specifically, you will contribute to the development of the Beam, Plasma & Accelerator Simulation Toolkit by working in the WarpX team, an open-source massively-parallel particle-in-cell code that was awarded the prestigious 2022 ACM Gordon Bell Prize. You will participate in the advancement of theoretical and computational plasma and beam accelerator physics, through one or more of the various activities that occur in the program, which offers a wide range of possibilities such as: Investigating physics through simulation campaigns to support theoretical and/or experimental studies with the codes developed in the program; Improving our computational tools or methods for better performances on supercomputers, or for simulating new physics; Exploring novel numerical schemes, algorithms, and AI/ML approaches to improve simulations’ reliability.","* Python and/or C++ development * improve GPU-accelerated HPC open source codes * AI/ML * supercomputers such as Perlmutter (NERSC) * Python notebooks & presentations for demonstration/training/debugging. A background/study in physics, computer science or applied math is desired and the focus of the project will be adjusted accordingly. To stand out from the crowd, you might already have experience with Python and C++, know how to use GitHub, or have run simulations before.",,Meet out team at LBNL and the Exascale code WarpX,particle; accelerator; hpc; plasma; physics; fusion; particle-in-cell;,"Arianna Formenti is a post-doc in the Accelerator Technology and Applied Physics Division at Lawrence Berkeley National Laboratory. Her research interests include modeling of future particle colliders – such as Higgs factories and 10 TeV-level wakefield colliders – and fusions plasmas with advanced simulations. She is a maintainer of the code WarpX, which is part of the BLAST simulation toolkit. She received a BSc and MSC in Mathematical Engineering and a PhD in Energy and Nuclear Science and Technology from Politecnico di Milano, Italy. Arianna is also committed to science education, workforce development, and outreach."
doeproj104s1,Accept (Unconfirmed),Tianyi,Shi,tianyishi@lbl.gov,Lawrence Berkeley National Laboratory,,,,Develop tensor algorithms in Python,,,Applied Computer Science; Applied Mathematics; High Performance Computing; Open Source Software,"A wide range of applications involve multidimensional data as observations or solutions, and these data sets are often referred to as tensors. The storage cost of tensors grows exponentially with respect to the dimensionality, also known as ""the curse of dimensionality"", so researchers develop various data-sparse tensor formats for lower storage and faster computations. This project focuses on a special tensor format called the tensor-train (TT) format. We have developed some data-centric algorithms to factor a given tensor into TT representations. Our proposed algorithms can also be parallelized, and we can use them in analyzing large data sets, solving partial differential equations, and conducting statistical learning. In order to compare with state-of-the-art data-centric TT decomposition packages, we would like to implement our algorithms in Python, specifically with PyTorch and MPI4Py, using highly optimized linear algebra kernels and distributed memory parallelism. Then we can perform a thorough comparison between our proposed algorithms and the existing ones.","Interests or background in numerical linear algebra and parallel computing. Desired skills: know how to code in Python, preferably PyTorch.",,Tensor Computations in Python,Numerical linear algebra; tensor computations; parallel algorithms; Python.,"Tianyi Shi is a postdoctoral scholar at Lawrence Berkeley National Laboratory in the Scalable Solvers Group within the Computing Sciences Area. He received his Ph.D. in Applied Mathematics from Cornell University. Tianyi’s research focuses on developing efficient and scalable algorithms for sparse and data-sparse matrices and tensors, with applications in computational chemistry and high-performance computing. His work involves designing shared- and distributed-memory parallel CPU and GPU codes in C, C++, and CUDA, and conducting large-scale experiments on supercomputers."
doeproj105s1,Accept (Unconfirmed),Phillip,Thomas,pthomas@lbl.gov,Lawrence Berkeley National Laboratory,NERSC,,,Simulating Vibrational Spectra of Molecules,,,Applied Computer Science; Applied Mathematics; High Performance Computing; Materials Engineering; Open Source Software; Software Engineering,"Our group is interested in simulating infrared spectra of molecules from first principles calculations. We have a code, called ""MLCP"", which solves the high-dimensional eigenvalue problem to generate the energy levels. We are interested in building an automated simulation pipeline which integrates MLCP with other tools to rapidly simulate vibrational spectra of new systems.",High Performance Computing Programming Chemistry and/or physics,,Simulating Vibrational Spectra of Molecules,Fortran; python; spectroscopy; chemistry; physics; high performance computing; simulation; workflow; pipeline; automation,"Phillip is an Application Performance Engineer at NERSC. His areas of expertise include Fortran, GPU programming, and algorithms related to tackling the Curse of Dimensionality. His efforts at NERSC involve improving the performance of material science applications. Before coming to NERSC, Phillip developed compute kernels for Wave Computing in Santa Clara, California. Prior to that Phillip worked as a post-doctoral researcher in quantum reaction dynamics at Queen's University in Ontario, Canada and at Leiden University in the Netherlands. He also has experience in experimental molecular spectroscopy from a post-doctoral fellowship at The Ohio State University. Phillip's Ph.D. thesis topic was spectroscopy and simulation of reactive organic molecules."
doeproj106s1,Accept (Unconfirmed),Kevin A.,Brown,kabrown@anl.gov,Argonne National Laboratory (ANL),Mathematics and Computer Science Division,,https://www.anl.gov/profile/kevin-a-brown,Analyzing and Modeling Large Scale Infrastructure,,,"Artificial Intelligence and Intelligent Systems; Computer Science; Electrical, Electronic, and Information Engineering; High Performance Computing; Informatics, Analytics and Information Science; Infrastructure and Instrumentation; Other Computer and Information Sciences; Performance Evaluation and Benchmarking; Software Engineering; Statistics and Probability","Large-scale computing infrastructure—including supercomputers and wide-area research networks—underpins breakthroughs in science and engineering. Yet the scale and complexity of these systems make them difficult to deploy, operate, and optimize. They produce massive volumes of telemetry and logs that must be distilled into actionable insight, and faults can cascade across subsystems, degrading reliability and productivity if not detected and mitigated quickly. Our work brings together complementary capabilities to study, design, and optimize these infrastructures: • System and workload modeling that combines AI, parallel discrete-event simulation (PDES), and fast surrogate models • Scalable log and telemetry analysis, including automated, AI-assisted pipelines • Anomaly detection and failure prediction to improve resilience • Network performance benchmarking, measurement, and analysis to guide planning and operations",- Basic programming experience is an asset,,Analyzing and Modeling Large-Scale Infrastructure,networking supercomputing HPC ESnet AI Artificial intelligence Resilience Failure analysis Performance analysis,"Kevin A. Brown is the Assistant Computer Scientist at Argonne National Laboratory where he investigates new designs for supercomputer networks. He received his B.Sc. from the University of Technology, Jamaica (UTech) and his M.Sc. and Ph.D. in Mathematical and Computing Science from the Tokyo Institute of Technology. His prior work experience includes serving as a Unix Systems Administrator at Digicel (Jamaica) Ltd. and as a postdoctoral appointee in the Argonne Leadership Computing Facility focused on exascale interconnect performance evaluation."
doeproj107s1,Accept (Unconfirmed),Sai Krishna Kanth,Hari,hskkanth@lanl.gov,Los Alamos National Laboratory,,,,Scalable Algorithms for Critical Infrastructure Network Planning,,,"Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; Chemical Engineering; Civil Engineering; Computer Science; Electrical, Electronic, and Information Engineering; High Performance Computing; Hydrology and Water Resources; Infrastructure and Instrumentation; Mechanical Engineering","Critical infrastructure networks—such as power grids, water distribution systems, and natural gas pipelines—form the backbone of modern society. Planning their design, daily operation, and expansion requires repeatedly solving large-scale, complex optimization problems. These problems are computationally challenging due to nonlinear and nonconvex flow models, discrete decision variables, and the vast scale of network systems. This project focuses on developing scalable optimization algorithms for such critical infrastructure systems. We leverage techniques such as linear relaxations, spatial and temporal decomposition, and problem-structure exploitation to design methods capable of handling realistic, high-fidelity flow models efficiently. The outcomes aim to enable faster and more reliable decision-making in infrastructure planning and operation—directly supporting the Department of Energy’s mission to enhance the resilience, efficiency, and security of national energy and water networks.","Interest in optimization algorithms, energy or network systems, or applied mathematics; experience with Julia, Python, C++, or MATLAB.",,Scalable Algorithms for Critical Infrastructure Network Planning,"Optimization, critical infrastructure, energy systems, network modeling, algorithms, decomposition, applied mathematics, computational methods.","Research scientist with interests in critical infrastructure network planning, autonomous vehicle planning and applied optimization. Background in civil and mechanical engineering."
doeproj108s1,Accept (Unconfirmed),Kesheng (John),Wu,kwu@lbl.gov,Lawrence Berkeley National Laboratory (LBNL),,,https://profiles.lbl.gov/20161-john-wu/,Advanced Modeling for Prediction and Anomaly Detection in Network Operations,,,Applied Computer Science; Artificial Intelligence and Intelligent Systems; Infrastructure and Instrumentation,"As we approach the summer of 2026, our goal is to develop and deploy advanced modeling capabilities for predicting hardware failures and detecting external configuration anomalies in our network infrastructure. This project aims to leverage machine learning and data analytics to improve network stability, reduce unplanned outages, and enhance overall network efficiency. We will focus on two key areas: (1) Predicting Hardware Failures: By analyzing log and telemetry data, we will develop models to predict hardware failures in networking equipment, enabling proactive replacement and minimizing downtime. (2) Detecting Network Configuration Anomalies: We will create a tool to analyze received network configuration data, identify routing path changes, and notify engineers of anomalies, ensuring swift response to potential disruptions. Through this project, we will apply advanced modeling techniques to drive business outcomes, including: - Improved network stability and reduced unplanned outages - Enhanced predictive capabilities for hardware failures and configuration anomalies - Data-driven decision-making for network operations and maintenance By investing in advanced modeling and analytics, we will take a proactive approach to network operations, ensuring a more resilient and efficient network infrastructure for the summer of 2026 and beyond.",Strong analytics skills are must. Computer networking knowledge is desired.,,Advanced Modeling for Prediction and Anomaly Detection in Network Operations,Predictive Modeling Anomaly Detection Network Stability Machine Learning Data Analytics Network Operations,"Dr. Kesheng (John) Wu leads multiple R&D endeavors focused on advanced technologies and testbeds at the Scientific Networking Division of Lawrence Berkeley National Laboratory. These projects aim to expedite data transfer among DOE user facilities, implement in-network storage and computational resources for intricate scientific workflows, and explore algorithms, strategies, and practices to enhance the efficiency of network operations. Additionally, Dr. Wu's team is tasked with developing and managing networking testbeds, providing the broader research community with platforms to explore future generations of networking technologies and optimize their utilization. These testbeds encompass conventional optical networking alongside cutting-edge quantum communication capabilities."
doeproj109s1,Accept (Unconfirmed),Talita,Perciano,tperciano@lbl.gov,Lawrence Berkeley National Laboratory,Scientific Data Division,,,AQuA-DATA: Advanced Quantum Algorithms for Diverse Applications and Theoretical Advancements in Science,,,Applied Mathematics; Computer Science; Other Computer and Information Sciences,"This project aims to bridge the gap between theoretical quantum advantages and practical scientific applications by developing quantum algorithms and quantum machine learning methods, focusing on efficient data encoding to achieve quantum utility across diverse scientific domains. Our proposal centers on developing a sophisticated suite of quantum algorithms tailored to harness classical data for advanced scientific applications. We emphasize efficient quantum data encoding, coupled with error mitigation approaches, as a pivotal strategy to bridge the gap between theoretical quantum capabilities and practical use across diverse scientific domains. Targeting classical data, which parallels the concept of classical channels in quantum physics, our approach involves specific data reduction techniques and sparsity to refine and simplify the data to serve as input to our quantum algorithms. This makes it more amenable to quantum processing, particularly focusing on computationally intensive components of larger data analysis pipelines. Our methods encompass both pure quantum and hybrid quantum-classical algorithms.","Computer Science, software development, quantum algorithms, quantum computing, applied math",,Machine Learning and Quantum Algorithms for Science,Machine learning; applied math; quantum algorithms; probabilistic graphical models; high performance computing.,"Dr. Perciano is a Research Scientist in the AI & Learning Systems group and the Computational Biosciences group, at Lawrence Berkeley National Laboratory. She conducts research in the areas of quantum computing, quantum algorithms, image analysis, machine learning, probabilistic graphical models, and high-performance computing motivated by the incredible challenges around scientific data generated by computational models, simulations, and experiments. Her research focuses on mathematical foundations for new methods, on the implementation of scalable methods, and on platform-portability. Her goal is to develop powerful, mathematically-grounded, scalable algorithms that meet the requirements needed to analyze current and future scientific datasets acquired in user data facilities. She has built a diverse collaboration network throughout the years in fields such as materials science, biosciences, chemistry, among others."
doeproj110s1,Accept (Unconfirmed),Arjun,Sharma,asharm1@sandia.gov,Sandia National Labs,Applied and Computational Mathematics,,,AI + Physics for Better Propellers/ turbines,,,Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; Environmental Engineering; Fluid and Plasma Physics; High Performance Computing; Mechanical Engineering; Open Source Software; Performance Evaluation and Benchmarking; Software Engineering,"Propellers and turbines, spinning blades on drones, aircraft, ships, and wind turbines, have been modeled since before powerful computers. Engineers built quick “low-order” tools such as blade-element models (slice each blade into small segments), actuator-line models (represent the blade as a line of force), and vortex methods (capture swirling wakes). These tools are fast and great for early design exploration but can miss important flow features. By contrast, high-fidelity computational fluid dynamics (CFD) resolves those details but is often too slow for everyday use. This project blends the best of both worlds. We keep the fast physics model and train a neural network to learn the missing piece, the difference between low-order predictions and high-fidelity results. This “grey-box” approach respects physics while data improves it. We focus on practical performance measures, how much push (thrust) and how hard to spin (torque), rather than every pressure or velocity detail, keeping the method simple for early design. Summer work may include building a benchmark dataset, designing and training neural network, visualizing results, and packaging a reusable tool. Students can focus on coding, physics, or visualization based on interest. The outcome: better early designs for aircraft, ships, and clean-energy systems.","• Curiosity about physics and data • Some programming (Python or MATLAB—or any language) • Basic data handling/plotting (matplotlib, or similar) • Comfort with algebra/calculus; willingness to learn new tools or mathematics • Willingness to learn, ask questions, and work as a team Nice to have: basic ML; exposure to aerodynamics/CFD; familiarity with HPC clusters; interest in aircraft, drones, or clean energy.",,"Faster, Trustworthy Fluids: Al + physics for Propellers, Wings, Aerosols.","propellers, turbines, drones, aircraft, airfoils, aerosols, droplets, ice, turbulence, aerodynamics, thrust, torque, machine-learning, neural-networks, design optimization, climate, air-quality, SINDy","Arjun Sharma is a Postdoctoral Appointee at Sandia National Laboratories (Computer Science Research Institute). He works at the intersection of fluid mechanics, computing, and machine learning, building fast, trustworthy models for design and discovery. Recent projects include improving classic wing aerodynamics with learned corrections, non local transport of nutrients in bacterial suspension, air-sea flux exchange in the DOE’s climate model, and machine learning physics parametrization for climate models. He codes in Python (JAX/PyTorch), Matlab and Fortran, uses GPUs, and runs larger studies on HPC clusters, with an emphasis on clear, reproducible workflows and understanding physical mechanisms. Arjun earned a Ph.D. in Mechanical Engineering (Applied Mathematics minor) from Cornell University. Before graduate school he worked in high-performance engineering at Rolls-Royce (aeroacoustics) and in Formula One (aerodynamics). He has taught and TA’d core courses in thermodynamics and fluid dynamics and enjoys mentoring students at different levels. For SRP, Arjun offers flexible projects where students can lean into coding, physics, or visualization and leave with a portfolio-ready artifact (well-documented repo, figures, and a short write-up). His mentoring style is supportive and structured, weekly one-on-ones, code reviews, and clear milestones, with opportunities for co-authorship when results warrant"
doeproj111s1,Accept (Unconfirmed),Justin,Wozniak,woz@anl.gov,Argonne National Laboratory,Data Science & Learning,,https://www.anl.gov/profile/justin-m-wozniak,Applying Performance Prediction to Live Filesystems,,,Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; Open Source Software; Performance Evaluation and Benchmarking; Visualization and Human-Computer Systems,"Large shared filesystems can be an unpredictable environment for running large scientific workflows. Our team has developed a performance collection and prediction software package. We would now like to improve the software, make it more portable and easy to use, and run it on more real-world filesystems. The team in this project will run the data collection and prediction tools on live systems with real applications. The research aspect will be in how to interpret and visualize the resulting data. Possible applications include light source experiments from the Advanced Photon Source and data management workflows from epidemics modeling.",Linux Shell scripting Machine learning,,Prediction and resilience for distributed workflows,workflows;hpc;filesystems;prediction,"Wozniak designs and implements workflow systems for scientific applications and deep learning workloads, combining techniques from high-performance computing and distributed computing. He has extensive experience integrating advanced computing techniques in collaboration with the health, experimental, and simulation sciences."
doeproj112s1,Accept (Unconfirmed),Bert,de Jong,wadejong@lbl.gov,Lawrence Berkeley National Laboratory,,,,AI agents for chemical and materials science,,,Chemical Engineering; Computer Science; Condensed Matter Physics,"Discovering new materials or chemical processes is a complex and time-consuming endeavor, often requiring countless experiments, intricate analysis, and sometimes even serendipitous discoveries. Inverse design has the promise to rationally discover new materials and processes. Supported by artificial intelligence, inverse design can dramatically accelerate this process by analyzing vast datasets, predicting material properties, and guiding experimental design—reducing both the time and uncertainty involved in scientific discovery. Our team is working on developing agents, integrated with machine learning and large language models, self-driving experiments and HPC simulations.","Physical chemistry or materials knowledge would be helpful Good understanding of AI and ML methods, some knowledge of agentic AI.",,AI agents for chemical and materials science,AI; ML; chemical processes; materials; discovery; inverse design,"Bert de Jong is the Director of the Quantum Systems Accelerator, which is part of the National Quantum Initiative. In addition, de Jong is the Team Director of the Accelerated Research for Quantum Computing (ARQC) Team MACH-Q, funded by DOE ASCR, focused on developing software stacks for near-term quantum computing devices. In addition, de Jong has a program in AI and machine learning to understand biomolecular processes, and discover new materials and molecular crystals for gas adsorption. de Jong serves as the Department Head for Computational Sciences, and leads the Applied Computing for Scientific Discovery Group, which advances scientific computing by developing and enhancing applications in key disciplines, as well as developing HPC, quantum and AI tools and libraries for addressing general problems in computational science."
doeproj113s1,Accept (Unconfirmed),Graham,Harper,gbharpe@sandia.gov,Sandia National Laboratories,,,,Multilevel Machine Learning for Material Modeling,,,Applied Computer Science; Applied Mathematics; High Performance Computing; Materials Engineering,"Multilevel methods are a class of algorithms which accelerate solutions of large problems by solving a sequence of smaller problems. Recent developments have shown multilevel methods may be used to train large neural networks by instead training a sequence of smaller networks. The benefits of these approaches are derived from the comparatively lower costs of training smaller networks. This is particularly effective for Kolmogorov-Arnold Networks (KANs), which are a type of neural network which learn individual activation functions. This also allows for development of mathematical theory related to polynomial approximation of functions. These theories and algorithms may be used to accelerate training of neural networks for a variety of different applications, such as material modeling engineering applications.",Mathematics theory Machine learning software and theory Computer science and programming Engineering and materials,,Multilevel Training for Machine Learning,Mathematics; Multilevel Methods; Machine Learning; Neural Networks; Kolmogorov-Arnold Networks; Computer Science; Material Modeling; Engineering;,"Graham Harper is a staff member at Sandia National Laboratories in the scientific machine learning department. He has expertise in finite element methods, domain decomposition, linear solvers, multigrid, earth system modeling, and machine learning. He has developed high-performance software for several libraries, including Trilinos, deal.II, and Neko. In his free time, Graham enjoys 3D printing, electronics repair, and gardening."
doeproj114s1,Accept (Unconfirmed),Roel,Van Beeumen,rvanbeeumen@lbl.gov,Lawrence Berkeley National Laboratory,Applied Mathematics and Computational Research Division,,https://www.roelvanbeeumen.be,Simulation of quantum algorithms,,,Applied Mathematics; Computer Science; Condensed Matter Physics; High Performance Computing; Open Source Software; Performance Evaluation and Benchmarking,"Current and near-term quantum computers, also known as noisy intermediate-scale quantum (NISQ) computers, are characterized by low qubit counts, short qubit decoherence times, and high gate error rates. On the other hand, rapid progress in both quantum hardware and software results in continuous simulation needs of novel/modified quantum algorithms. The QCLAB and QCLAB++ simulation software packages, developed at LBNL, are respectively an object-oriented MATLAB toolbox and a fully templated C++ package for constructing, representing, and simulating quantum circuits. Designed with an emphasis on numerical stability, efficiency, and performance, QCLAB provides a reliable platform for prototyping and testing quantum algorithms. For advanced performance needs, QCLAB++ serves as a complementary C++ package optimized for GPU-accelerated quantum circuit simulations. Together, QCLAB and QCLAB++ form a comprehensive toolkit, balancing the simplicity of MATLAB scripting with the computational power of GPU acceleration. Project ideas could include improving (CPU/GPU) performance, adding different noise models, implementing qudits functionalities, and expanding the quantum algorithms' base.","Specific background depends on the specific project focus, but generally: software development (Matlab, C++), quantum computing, applied mathematics, etc.",,Quantum Computing meets Numerical Linear Algebra,Quantum Computing Quantum Algorithms NISQ Quantum Linear Algebra Matlab C++ CPU/GPU Performance Optimization State Vector Simulation,"Roel Van Beeumen is a Staff Scientist in the Applied Mathematics and Computational Research Division at Berkeley Lab. His research interests range from numerical linear algebra and software for solving large-scale and high dimensional eigenvalue problems to quantum computing and quantum algorithms. He earned his PhD in Engineering Science: Computer Science (2015) at KU Leuven in Belgium, from which he also holds Master degrees in Mathematical Engineering (2010) and in Archaeology (2011)."
doeproj115s1,Accept (Unconfirmed),Zhe,Bai,zhebai@lbl.gov,Lawrence Berkeley National Laboratory,Applied Mathematics and Computational Research,,https://profiles.lbl.gov/39980-zhe-bai,Sequential modeling of multi-scale dynamical systems,,,Applied Computer Science; Applied Mathematics; Artificial Intelligence and Intelligent Systems; High Performance Computing,"This project develops advanced sequential models to analyze time-series data. The goal is to capture the complex, multi-scale, path-dependent relationships using historic event data. The model is expected to learn both short-term and long-term temporal patterns for an overall reliable forecast.",Proficient in coding; experienced in optimizing neural networks; interested in transformers and/or high performance computing.,,Sequential modeling of multi-scale dynamical systems,Advanced sequential modeling; hierarchical learning; time series forecasting; dynamical systems.,"Zhe Bai is a computational science researcher in the Applied Mathematics and Computational Research Division of the Computing Sciences Area at Lawrence Berkeley National Laboratory. Her research interests lie in the area of data-driven modeling and model order reduction, including dynamic mode decomposition, scientific machine learning and sparse algorithms for large-scale computation and simulation. Cultivated interdisciplinary research and collaborations spanning the fields of computational science and engineering, her work focuses on AI-based modeling that couples first-principles with data-driven approaches to understand, estimate and control high-dimensional physical systems."
doeproj116s1,Accept (Unconfirmed),Kathryn,Maupin,kmaupin@sandia.gov,Sandia National Laboratories,Optimization and Uncertainty Quantification,,,Uncertainty Quantification of Displacement Damage Models,,,Applied Computer Science; Applied Mathematics; Computer Science; Other Computer and Information Sciences; Particle and High-Energy Physics; Statistics and Probability,"As the third pillar of science, computational simulation has allowed scientists to explore, observe, and test physical regimes previously thought to be unattainable. High-fidelity models are derived from physical principles and calibrated to experimental data. However, missing or unknown physics and measurement, experimental, and numerical errors give rise to uncertainties in the model form and parameter values in even the most trustworthy models. Thus, rigorous calibration and validation of a computational model is paramount to its effective us as a predictive tool. The popularity of the Bayesian paradigm stems from its natural integration of measurement and model uncertainties. A systematic approach to model validation, progressing from parameter and quantity of interest identification to sensitivity analysis, calibration, and validation, is applied to a drift-diffusion simulation code called Charon. Charon allows the computational qualification of semiconductor devices subjected to displacement damage. *Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525.","Coding (any language), Command Line Interface, Desire to learn",,Uncertainty Quantification of Displacement Damage Models,Bayesian methods; model calibration; model validation; uncertainty quantification; model discrepancy; model form error.,"Kathryn Maupin is a Principal Member of the Technical Staff at Sandia National Laboratories. Motivated by a passion for transforming uncertainty into actionable insights, Kathryn leverages her extensive expertise in model validation, model form error quantification, and Bayesian analyses to drive innovative solutions that enhance research outcomes. Kathryn earned her PhD in Computational Science, Engineering, and Mathematics, along with her M.S. in Computational and Applied Mathematics, both from The University of Texas at Austin. Her fascination with mathematical modeling began at the University of California, San Diego, where she completed her B.A. in Applied Mathematics. When she is not immersed in data and algorithms, Kathryn enjoys the chaos of family life with her three children and three dogs. Looking ahead, Kathryn aspires to continue pushing the boundaries of computational science while encouraging others to confront ubiquitous uncertainty in their work."